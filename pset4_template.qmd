---
title: "Problem Set 4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (Ella Montgomery; emontgomery2):
• Partner 2 (Mitch Bobbin; mbobbin):
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **EM** **MB**
5. “I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” (1 point)
6. Late coins used this pset: **__** Late coins left after submission: **__**

## Download and explore the Provider of Services (POS) file (10 pts)

1. We chose to include the following variables:
- Hospital Type: **PRVDR_CTGRY_SBTYP_CD**,  **PRVDR_CTGRY_CD**
- CMS Certification number: **PRVDR_NUM**
- Facility name: **FAC_NAME**
- Termination code: **PGM_TRMNTN_CD**
- ZIP Code: **ZIP_CD**
- State code: **STATE_CD**

2. 
    a.
    ```{python}
    import pandas as pd
    import os
    #only need to change base path when we're switching off working
    base_path=r"C:\\Users\\EM\\Documents\\GitHub\\problem-set-4-mitchella"

    file_path2016=r"pos2016.csv"

    path2016=os.path.join(base_path,file_path2016)
    
    #import the data for 2016 and store in a df
    df_pos2016=pd.read_csv(path2016)

    df_pos2016.shape
    df_pos2016.groupby("PRVDR_CTGRY_SBTYP_CD").count()

    #filter to the correct provider category and subcategory:

    df_pos2016=df_pos2016[(df_pos2016["PRVDR_CTGRY_SBTYP_CD"]==1) & (df_pos2016["PRVDR_CTGRY_CD"] == 1)]

    df_pos2016.shape

    #take the number of rows from the filtered dataset over the
    #number of rows in the unfiltered to get the proportion that
    #are short-term hospitals:

    7245/141557

    ```

There are 7,245 short term hospitals in this data. This represents about 5% of all providers in the data.



    b.
    This number may not make sense. This is a huge overestimate of the Kaiser Family Foundation's figure of around 5,000 "short-term acute hospitals", which they also published in 2016. It could differ because the two sources classify short term hospitals differently, with the Kaiser Family Foundation having a stricter definition than Medicare and Medicaid.
3. 

```{python}
file_path2017=r"pos2017.csv"

path2017=os.path.join(base_path,file_path2017)
    
#import the data for 2016 and store in a df
df_pos2017=pd.read_csv(path2017)

#filter to the correct provider category and subcategory:

df_pos2017=df_pos2017[(
  df_pos2017["PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2017["PRVDR_CTGRY_CD"] == 1)]

df_pos2017.shape
```

7,260 short term hospitals in 2017.

```{python}
file_path2018=r"pos2018.csv"

path2018=os.path.join(base_path,file_path2018)
    
#import the data for 2018 and store in a df
#for some reason we needed to specify encoding. used chatgpt
#to troubleshoot when it wouldnt load in.
df_pos2018=pd.read_csv(path2018,encoding="latin1")

#@Ella I think you and I have different names for the subgroup #column when we've exported. for some reason mine is loading 
#this way
df_pos2018=df_pos2018[(
  df_pos2018["ï»¿PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2018["PRVDR_CTGRY_CD"] == 1)]

df_pos2018.shape

```

7,277 hospitals in 2018.

```{python}
file_path2019=r"pos2019.csv"

path2019=os.path.join(base_path,file_path2019)
    
#import the data for 2019 and store in a df
#for some reason we needed to specify encoding. used chatgpt
#to troubleshoot when it wouldnt load in.
df_pos2019=pd.read_csv(path2019,encoding="latin1")

df_pos2019=df_pos2019[(
  df_pos2019["ï»¿PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2019["PRVDR_CTGRY_CD"] == 1)]

df_pos2019.shape
```

7,303 hospitals in 2019.

```{python}
#add each dfs associated year to every observation:
df_pos2016["year"]=2016
df_pos2017["year"]=2017
df_pos2018["year"]=2018
df_pos2019["year"]=2019
#combine the dfs
combined_df_pos=pd.concat([df_pos2016,df_pos2017,df_pos2018,df_pos2019])

#plot the combined df
import altair as alt
alt.data_transformers.enable("vegafusion")

observation_plot=alt.Chart(combined_df_pos).mark_bar().encode(
  alt.X("year:O"),
  alt.Y("count()")
)

```


4. 
    a.
```{python}
#gives the number of unique values for PRVDR_NUM by year:
print(combined_df_pos.groupby("year")["PRVDR_NUM"].nunique())

#plot
alt.data_transformers.enable("vegafusion")

hospital_plot=alt.Chart(combined_df_pos).mark_bar().encode(
  alt.X("year:O"),
  alt.Y("distinct(PRVDR_NUM):Q")
)

#display both plots
hospital_plot | observation_plot
```


    b. 
    The plots show us that each individual row is a single hospital, because the plots are identical. In the first plot, we examined the number of rows for each year, and in the second, we examined the number of unique values for PRVDR_NUM, and they appear to be identical.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#active in 2016
active_2016 = combined_df_pos[(combined_df_pos['year'] == 2016) & (combined_df_pos['PGM_TRMNTN_CD'] == 0)]

#compare against following years
merged = active_2016[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].merge(
    combined_df_pos,
    on='PRVDR_NUM',
    how='left'
)

#filter out active hospitals
closed_hospitals = merged[(merged['year'] > 2016) & (merged['PGM_TRMNTN_CD'] == 1)
]

print(closed_hospitals['PRVDR_NUM'].nunique())
```

There are 133 hospitals that fit this suspected closure definition.

2. 

```{python}
print(closed_hospitals[['FAC_NAME_x', 'year']].sort_values(by='FAC_NAME_x').head(10))
```

3. 

```{python}
closed_hospitals = closed_hospitals[['PRVDR_NUM', 'FAC_NAME_x', 'ZIP_CD_x', 'PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD', 'STATE_CD', 'PGM_TRMNTN_CD', 'year']]

closed_hospitals.rename(columns={'FAC_NAME_x': 'FAC_NAME', 'ZIP_CD_x': 'ZIP_CD'}, inplace=True)

#yearly count of active hospitals by ZIP code
yearly_zip_active_pos = combined_df_pos[combined_df_pos['PGM_TRMNTN_CD'] == 0].groupby(['year', 'ZIP_CD']).size().reset_index(name='active_pos')


#do we need to do the same test with looking at from 16-17?
#what if the zip code has 1 active hospital one year, and then 0
#in the following year. Is there anything to group on for that
#year and can we detect that closure in our data?

#can we amend our yearly_zip_active_pos df to look at the zips
#where it goes from 1 active to 0, and add the missing years
#along with the number 0 for active_pos. This is essential
#because we won't be able to note zip codes that decrease if
#they just drop out of the dataset.

all_years = yearly_zip_active_pos['year'].unique()
all_zip_codes = yearly_zip_active_pos['ZIP_CD'].unique()
all_combinations = pd.MultiIndex.from_product([all_years, all_zip_codes], names=['year', 'ZIP_CD']).to_frame(index=False)

# Merge with the existing data to include missing combinations
complete_yearly_zip_active_pos = all_combinations.merge(yearly_zip_active_pos, on=['year', 'ZIP_CD'], how='left')

# Fill missing 'active_pos' values with 0 for cases without observations
complete_yearly_zip_active_pos['active_pos'] = complete_yearly_zip_active_pos['active_pos'].fillna(0)

#identify zip codes where number of hospitals is changing

zip_changes = complete_yearly_zip_active_pos.groupby("ZIP_CD")['active_pos'].nunique().reset_index()
complete_yearly_zip_active_pos = complete_yearly_zip_active_pos.sort_values(['ZIP_CD', 'year'])

#Calculate the difference in active_pos by year within each ZIP code
complete_yearly_zip_active_pos['active_pos_diff'] = complete_yearly_zip_active_pos.groupby('ZIP_CD')['active_pos'].diff()

#Identify ZIP codes with any decrease in active_pos over time.
#If there's a zip code with a decrease, we can assume the closure
#is "real" and not due to a M&A
zip_codes_with_decrease = complete_yearly_zip_active_pos[complete_yearly_zip_active_pos['active_pos_diff'] < 0]['ZIP_CD'].unique()

#Filter the closed_hospitals DataFrame for those in ZIP codes with a decrease in active_pos
actual_closures = closed_hospitals[closed_hospitals['ZIP_CD'].isin(zip_codes_with_decrease)]

len(actual_closures["PRVDR_NUM"].unique())

#168 of the 174 are actual closures. That means 6 are M&As


```

    a. There are 22 potential mergers.
    b. 111 hospitals are suspected to be an actual closure, due to our methodology of filtering out any hospital that was in a zip code where they didn't experience a decrease in the number of hospitals.

    c.
```{python}
print(actual_closures.sort_values("FAC_NAME").head(10))

```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are as follows:
    - .dbf, which contains attribute informaion in a table format
    - .prj, which contains information about units and the Coordinate Reference System (CRS)
    - .shp, which contains feature geometry
    - .shx, which contains the positional index information
    - .xml, which is written in a markup language and contains plaintext metadata and plainkeys

    b. 
    - .dbf: 6275 kb
    - .prj: 1 kb  
    - .shp: 817915 kb
    - .shx: 259 kb
    - .xml: 16 kb 
2. 

```{python}
import geopandas as gpd

#shp_path = r"C:\\Users\\Mitch\\Documents\\GitHub\\problem-set-4-mitchella\\gz_2010_us_860_00_500k.zip"

#@Ella: I had to change the path a bit to get this to #run. Adjust as needed for your own purposes yours was:
shp_path = 'C:\\Users\\EM\\Documents\\GitHub\\problem-set-4-mitchella\\gz_2010_us_860_00_500k\\gz_2010_us_860_00_500k.shp'

geo_data = gpd.read_file(shp_path)

#TX ZIPs start with 75-79
tx_geo_data = geo_data[geo_data['ZCTA5'].astype(str).str.startswith(('75', '76', '77', '78', '79'))]
```

```{python}
import matplotlib.pyplot as plt

#group by zipcode and count number of unique POS numbers for each group
pos_per_zip = df_pos2016.groupby('ZIP_CD')['PRVDR_NUM'].nunique().reset_index()
pos_per_zip.columns = ['ZCTA5', '# of POS']

#convert ZIP from float to string
pos_per_zip['ZCTA5'] = pos_per_zip['ZCTA5'].astype(int)
pos_per_zip['ZCTA5'] = pos_per_zip['ZCTA5'].astype(str).str.zfill(5)

tx_pos_per_zip = pos_per_zip[pos_per_zip['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

#format zip codes
tx_geo_data['ZCTA5'] = tx_geo_data['ZCTA5'].astype(str).str.zfill(5)

tx_geo_data_1 = tx_geo_data.merge(tx_pos_per_zip, on='ZCTA5', how='left')

tx_geo_data_1.plot(column = '# of POS',
legend = True).set_axis_off()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
zips_all_centroids = geo_data.assign(centroid=geo_data.centroid)
zips_all_centroids.shape
```

The resulting GeoDataFrame is 33120 x 7. The columns are taken from census [documentation](https://www.census.gov/programs-surveys/geography/technical-documentation/records-layout/2010-zcta-record-layout.html#par_textimage_0):
- GEO_ID: Concatenation of 2010 State and County
- ZCTA5: ZIP Code Tabulation Area
- NAME: ZIP Code
- LSAD: legal/statistical area description
- CENSUSAREA: calculated area derived from the ungeneralized area of each ZIP Code
- geometry: polygonal shape of designated area
- centroid: center point of designated area

2. 

```{python}
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

border_ranges = ( '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884',  #New Mexico
'73', '74',  #Oklahoma
'716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729',  #Arkansas
'700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715',  #Louisiana
'75', '76', '77', '78', '79'
)
    
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith((border_ranges))]

print(zips_texas_centroids['ZCTA5'].nunique())
print(zips_texas_borderstates_centroids['ZCTA5'].nunique())
```

The subset zips_texas_centroids has 1,935 unique values, while the borderstates subset has 4057 unique entries. The latter subset includes Texan ZIP Codes as well.

3. 

```{python}
zips_with_hospitals = pos_per_zip[pos_per_zip['# of POS'] > 0]
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(zips_with_hospitals, on='ZCTA5', how='inner')
```

For this case, an inner merge on the ZIP Code variable was used to exclusively retain ZIP Codes that have more than one hospital.

4. 
    a.
    
    ```{python}
    import time
    import fiona

    #crs conversion: 
    #https://gis.stackexchange.com/questions/304061/identifying-coordinate-system-crs-from-projection-file-with-python
    #c = fiona.open("C:\\Users\\EM\\Documents\\GitHub\\problem-set-4-mitchella\\gz_2010_us_860_00_500k\\gz_2010_us_860_00_500k.shp")
    #crs = c.crs

    zips_texas_centroids = zips_texas_centroids.to_crs(epsg=4269)
    zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=4269)
    ztc_subset = zips_texas_centroids.head(10)


    start_time = time.time()

    subset_join = gpd.sjoin_nearest(ztc_subset, zips_withhospital_centroids,
    how='left', distance_col="distance")

    end_time = time.time()
    elapsed_time = end_time - start_time      
    ```

    There are 1935 observations in zips_texas_centroids, meaning that this operation with 10 observations would take 0.5% of the expected total time required for the full dataset.
    The time elapesed for the subsetted dataframe was 3.37 seconds, so the full dataframe is expected to take 651.67 seconds.

    b.
    
    ```{python}
    
    start_time = time.time()

    full_join = gpd.sjoin_nearest(zips_texas_centroids, zips_withhospital_centroids,
    how='left', distance_col="distance")

    end_time = time.time()
    elapsed_time = end_time - start_time   

    print(elapsed_time) 
    ```
    
    The join took 441 seconds, or 7.35 minutes. This is only about 68% of the original 651 second estimate; our original prediction was off by almost a third of total predicted elapsed time.

    c.
    The .prj file is in degrees (UNIT["Degree",0.017453292519943295]). This is about 1.2 miles.
5. 
    a.

```{python}
full_join = full_join.rename(columns={'ZCTA5_left': 'ZCTA5'})
avg_distances = full_join.groupby('ZCTA5')['distance'].mean().reset_index()
avg_distances = avg_distances.rename(columns={'distance': 'avg_pos_dist'})

full_join_avg = full_join.merge(avg_distances, on='ZCTA5')
```

   The EPSG:4269 conversion in the previous step left distance measured in degrees of lat/long.
    
    b. 

```{python}
full_join_avg['avg_pos_dist_miles'] = full_join_avg['avg_pos_dist'] * 69
full_join_avg['avg_pos_dist_miles'].head

```

  The conversion factor for degrees to miles is [about 69](https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles), so each degree measure is converted to miles by multiplication with this conversion factor. The nearest neighbor centroid calculation we did changed our data so that some ZIP codes have multiple nearest neighbors, and ZIP codes with multiple hospitals have several entries in the data with a distance from the hospital measure of 0. These two factors mean that the average provider of service distance column that was just added is useful for ZIP Codes that do not have a hospital, but redundant for areas with a hospital.

    c.

```{python}
full_join_avg.plot(column="avg_pos_dist_miles", legend=True).set_axis_off()
```

    
## Effects of closures on access in Texas (15 pts)

1. 

```{python}
#filter our actual_closures to only include tx zips:

affected_tx_zips=actual_closures[actual_closures["STATE_CD"]=="TX"]

#Each Zip code in Texas' number of closed hospitals:
print(affected_tx_zips.groupby("ZIP_CD").size().sort_values(ascending=False))


```



2.

```{python}
#We're going to match the totals for each zip_code to the 
#geo dataframe.
#First, create the df of the zips and their associated hospital
#closures:
num_tx_zip_closures=affected_tx_zips.groupby("ZIP_CD").size().sort_values(ascending=False).reset_index()
num_tx_zip_closures = num_tx_zip_closures.rename(columns={"ZIP_CD":"ZCTA5",0: "num_closures"})

#df is displaying a decimal and 0 at the end so trim that off every entry:

num_tx_zip_closures["ZCTA5"] = num_tx_zip_closures["ZCTA5"].astype(str).str.replace(".0", "")

tx_geo_data["ZCTA5"]=tx_geo_data["ZCTA5"].astype(str)

#merge the relevant dfs:

tx_geo_data_closures = tx_geo_data.merge(num_tx_zip_closures, on="ZCTA5", how="left")
print(tx_geo_data_closures.head())
#looks like there might be an issue after the merge. But since
#there's thousands of zips, in the geo df, and only 65 in in the
#affected df, this is to be expected that there's NaNs. Here's
#a check:
print(tx_geo_data_closures[tx_geo_data_closures["ZCTA5"]=="77598"])
#looks good. but need to replace NaNs with 0s:
tx_geo_data_closures["num_closures"] = tx_geo_data_closures["num_closures"].fillna(0)

tx_geo_data_closures.plot(column="num_closures",
legend="num_closures")

#can just use affected_tx_zips to id the unique zips listed:
len(affected_tx_zips["ZIP_CD"].unique())

```

There are 33 different zip codes in Texas that experienced at least 1 hospital closure during 2016-2019.

3. 

```{python}
#geodataframe of all affected zips:
tx_geo_data_direct=tx_geo_data_closures[tx_geo_data_closures["num_closures"]>0]

#need a different epsg since this one isn't able to use a 
#buffer. used chatgpt to find one for me that will work.
projected_gdf = tx_geo_data_direct.to_crs(epsg=5070)

#apply a 10-mile buffer and retain attributes
buffered_gdf = projected_gdf.copy()
buffered_gdf["geometry"] = projected_gdf.buffer(10 * 1609.34)

#convert back to EPSG:4269. will need it for the merge
buffered_gdf = buffered_gdf.to_crs(epsg=4269)

#spatial join, which will id the zones that the buffer touches:
tx_geo_data_indirect = gpd.sjoin(tx_geo_data_closures, buffered_gdf, how="inner", predicate="intersects")

#the resulting gdf has 2 ZIP_CD columns. The left one 
#comes from our tx_geo_data_closures, and represents the 
#zip codes that are intersected by the right gdfs buffer
#from the zipcodes directly affected.
tx_geo_data_indirect["ZCTA5_left"].nunique()
#609 unique in the left column. However, this includes
#our original 33 zips. Need to get rid of rows where
#both our ZCTA5's are in the direct zips. can filter on 
#the ZCTA_left, if they're in the direct's ZCTA, then drop them:

direct_zips=tx_geo_data_direct["ZCTA5"]
direct_zips=direct_zips.astype(str)

tx_geo_data_indirect["ZCTA5_left"]=tx_geo_data_indirect["ZCTA5_left"].astype(str)

tx_geo_data_indirect_only=tx_geo_data_indirect[~tx_geo_data_indirect["ZCTA5_left"].isin(direct_zips)]
#used chatgpt to clean up this syntax

print(tx_geo_data_indirect_only["ZCTA5_left"].nunique())
```

There are 576 zip codes indirectly affected by our 10 mile buffer zone.

4. 

```{python}

fig, ax = plt.subplots(figsize=(10, 10))

#Plot all ZIPs in white
tx_geo_data.plot(ax=ax, color="white", edgecolor="black", linewidth=0.5, label="All ZIPs")

#Plot the direct ZIPs in red
tx_geo_data_direct.plot(ax=ax, color="red", edgecolor="black", linewidth=0.5, label="Hospital Closed within Zip")
import matplotlib.patches as mpatches
#Plot the indirect ZIPs in yellow
tx_geo_data_indirect_only.plot(ax=ax, color="yellow", edgecolor="black", linewidth=0.5, label="Within 10 Miles of a Zipcode with a Closure")
handles = [
    mpatches.Patch(color="white", edgecolor="black", label="Unaffected by Hospital Closure"),
    mpatches.Patch(color="red", edgecolor="black", label="Hospital Closed within Zip"),
    mpatches.Patch(color="yellow", edgecolor="black", label="Within 10 Miles of a Zipcode with a Closure")
]
#Customize and show the legend
plt.legend(handles=handles,loc="upper right", fontsize='small',title="Legend")
ax.set_axis_off()
plt.show()
```


## Reflecting on the exercise (10 pts) 
Partner 1:

Partner 2: This modus operandi for measuring ZIP Codes affected by closures is fairly basic, only taking into account the distance of the area's centroid's distance from another ZIP code's centroid. The most common count of hospitals per ZIP Code is one, so using the coordinate points of idividual hospitals would give a more precise measure of distance without being too computationally onerous. 