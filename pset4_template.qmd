---
title: "Problem Set 4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (Ella Montgomery; emontgomery2):
• Partner 2 (Mitch Bobbin; mbobbin):
3. Partner 1 will accept the ps4 and then share the link it creates with their partner.
You can only share it with one partner so you will not be able to change it after your
partner has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **EM** **MB**
5. “I have uploaded the names of anyone else other than my partner and I worked with
on the problem set here” (1 point)
6. Late coins used this pset: **__** Late coins left after submission: **__**

## Download and explore the Provider of Services (POS) file (10 pts)

1. We chose to include the following variables:
- Hospital Type: **PRVDR_CTGRY_SBTYP_CD**,  **PRVDR_CTGRY_CD**
- CMS Certification number: **PRVDR_NUM**
- Facility name: **FAC_NAME**
- Termination code: **PGM_TRMNTN_CD**
- ZIP Code: **ZIP_CD**
- State code: **STATE_CD**

2. 
    a.
    ```{python}
    import pandas as pd
    import os
    #only need to change base path when we're switching off working
    base_path=r"C:\Users\EM\Documents\GitHub\problem-set-4-mitchella"

    file_path2016=r"pos2016.csv"

    path2016=os.path.join(base_path,file_path2016)
    
    #import the data for 2016 and store in a df
    df_pos2016=pd.read_csv(path2016)

    df_pos2016.shape
    df_pos2016.groupby("PRVDR_CTGRY_SBTYP_CD").count()

    #filter to the correct provider category and subcategory:

    df_pos2016=df_pos2016[(df_pos2016["PRVDR_CTGRY_SBTYP_CD"]==1) & (df_pos2016["PRVDR_CTGRY_CD"] == 1)]

    df_pos2016.shape

    #take the number of rows from the filtered dataset over the
    #number of rows in the unfiltered to get the proportion that
    #are short-term hospitals:

    7245/141557

    ```

There are 7,245 short term hospitals in this data. This represents about 5% of all providers in the data.



    b.
    This number may not make sense. This is a huge overestimate of the Kaiser Family Foundation's figure of around 5,000 "short-term acute hospitals", which they also published in 2016. It could differ because the two sources classify short term hospitals differently, with the Kaiser Family Foundation having a stricter definition than Medicare and Medicaid.
3. 

```{python}
file_path2017=r"pos2017.csv"

path2017=os.path.join(base_path,file_path2017)
    
#import the data for 2016 and store in a df
df_pos2017=pd.read_csv(path2017)

#filter to the correct provider category and subcategory:

df_pos2017=df_pos2017[(
  df_pos2017["PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2017["PRVDR_CTGRY_CD"] == 1)]

df_pos2017.shape
```

7,260 short term hospitals in 2017.

```{python}
file_path2018=r"pos2018.csv"

path2018=os.path.join(base_path,file_path2018)
    
#import the data for 2018 and store in a df
#for some reason we needed to specify encoding. used chatgpt
#to troubleshoot when it wouldnt load in.
df_pos2018=pd.read_csv(path2018,encoding="latin1")

df_pos2018=df_pos2018[(
  df_pos2018["PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2018["PRVDR_CTGRY_CD"] == 1)]

df_pos2018.shape

```

7,277 hospitals in 2018.

```{python}
file_path2019=r"pos2019.csv"

path2019=os.path.join(base_path,file_path2019)
    
#import the data for 2019 and store in a df
#for some reason we needed to specify encoding. used chatgpt
#to troubleshoot when it wouldnt load in.
df_pos2019=pd.read_csv(path2019,encoding="latin1")

df_pos2019=df_pos2019[(
  df_pos2019["PRVDR_CTGRY_SBTYP_CD"]==1) & (
    df_pos2019["PRVDR_CTGRY_CD"] == 1)]

df_pos2019.shape
```

7,303 hospitals in 2019.

```{python}
#add each dfs associated year to every observation:
df_pos2016["year"]=2016
df_pos2017["year"]=2017
df_pos2018["year"]=2018
df_pos2019["year"]=2019
#combine the dfs
combined_df_pos=pd.concat([df_pos2016,df_pos2017,df_pos2018,df_pos2019])

#plot the combined df
import altair as alt
alt.data_transformers.enable("vegafusion")
alt.Chart(combined_df_pos).mark_bar().encode(
  alt.X("year:O"),
  alt.Y("count()")
)

```


4. 
    a.
```{python}
#gives the number of unique values for PRVDR_NUM by year:
print(combined_df_pos.groupby("year")["PRVDR_NUM"].nunique())

#plot
alt.data_transformers.enable("vegafusion")
alt.Chart(combined_df_pos).mark_bar().encode(
  alt.X("year:O"),
  alt.Y("distinct(PRVDR_NUM):Q")
)
```


    b. 
    The plots show us that each individual row is a single hospital, because the plots are identical. In the first plot, we examined the number of rows for each year, and in the second, we examined the number of unique values for PRVDR_NUM, and they appear to be identical.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#active in 2016
active_2016 = combined_df_pos[(combined_df_pos['year'] == 2016) & (combined_df_pos['PGM_TRMNTN_CD'] == 0)]

#compare against following years
merged = active_2016[['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].merge(
    combined_df_pos,
    on='PRVDR_NUM',
    how='left'
)

#filter out active hospitals
closed_hospitals = merged[(merged['year'] > 2016) & (merged['PGM_TRMNTN_CD'] != 0)
]

print(closed_hospitals['PRVDR_NUM'].nunique())
```

There are 174 hospitals that fit this suspected closure definition.

2. 

```{python}
print(closed_hospitals[['FAC_NAME_x', 'year']].sort_values(by='FAC_NAME_x').head(10))
```

3. 

```{python}
closed_hospitals = closed_hospitals[['PRVDR_NUM', 'FAC_NAME_x', 'ZIP_CD_x', 'PRVDR_CTGRY_SBTYP_CD', 'PRVDR_CTGRY_CD', 'STATE_CD', 'PGM_TRMNTN_CD', 'year']]

closed_hospitals.rename(columns={'FAC_NAME_x': 'FAC_NAME', 'ZIP_CD_x': 'ZIP_CD'}, inplace=True)

#yearly count of active hospitals by ZIP code
yearly_zip_active_pos = combined_df_pos[combined_df_pos['PGM_TRMNTN_CD'] == 0].groupby(['year', 'ZIP_CD']).size().reset_index(name='active_pos')

#merge yearly active hospitals per zipcode and closed hospitals
merged_2_3 = pd.merge(closed_hospitals, yearly_zip_active_pos, on=['ZIP_CD', 'year'], how='inner')
#drops to 97

#create yearly count of hospital per zip code in wide data format
yearly_counts = merged_2_3.groupby(['ZIP_CD', 'year'])['PRVDR_NUM'].count().unstack(fill_value=0)

#compare year to year change in total hospitals per zipcode
yearly_counts['decreased17_18'] = (yearly_counts[2017] > yearly_counts[2018])  

yearly_counts['decreased18_19'] = (yearly_counts[2018] > yearly_counts[2019])

merged_2_3 = pd.merge(merged_2_3, yearly_counts, on='ZIP_CD', how='left') 

#potential mergers/acquisitions 
potential_mergers = merged_2_3[merged_2_3['decreased17_18'] & merged_2_3['decreased18_19'] == False]

corrected_closures = merged_2_3[merged_2_3['decreased18_19'] == True]

```

    a. There are 95 potential mergers.
    b. Two hospitals remain; in all other zipcodes, the total number of hospitals did not decrease from year to year.
    c.

```{python}
print(corrected_closures.sort_values(by='FAC_NAME').head(10)) 

```

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are as follows:
    - .dbf, which contains attribute informaion in a table format
    - .prj, which contains information about units and the Coordinate Reference System (CRS)
    - .shp, which contains feature geometry
    - .shx, which contains the positional index information
    - .xml, which is written in a markup language and contains plaintext metadata and plainkeys

    b. 
    - .dbf: 6275 kb
    - .prj: 1 kb  
    - .shp: 817915 kb
    - .shx: 259 kb
    - .xml: 16 kb 
2. 

```{python}
import geopandas as gpd
shp_path = 'C:\\Users\\EM\\Documents\\GitHub\\problem-set-4-mitchella\\gz_2010_us_860_00_500k\\gz_2010_us_860_00_500k.shp'

geo_data = gpd.read_file(shp_path)

#TX ZIPs start with 75-79
tx_geo_data = geo_data[geo_data['ZCTA5'].astype(str).str.startswith(('75', '76', '77', '78', '79'))]
```

```{python}

df_pos2016

```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
